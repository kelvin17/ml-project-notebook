{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYloyUA4zJ8xdHk3fpOCLL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kelvin17/ml-project-notebook/blob/main/video_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Dataloader"
      ],
      "metadata": {
        "id": "vHLIVvmg0Ebc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_dir = '/content/drive/MyDrive/project/ufc10/'\n",
        "model_output = '/content/drive/MyDrive/model_output/video_classification/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyoh8NoV4Ba1",
        "outputId": "1ec928c5-28a2-4792-9c77-1272bb807daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVbLiWmdzsrt"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms as T\n",
        "\n",
        "class FrameImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir='/work3/ppar/data/ucf101', split='train', transform=None):\n",
        "        self.frame_paths = sorted(glob(f'{root_dir}/frames/{split}/*/*/*.jpg'))\n",
        "        self.df = pd.read_csv(f'{root_dir}/metadata/{split}.csv')\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frame_paths)\n",
        "\n",
        "    def _get_meta(self, attr, value):\n",
        "        return self.df.loc[self.df[attr] == value]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frame_path = self.frame_paths[idx]\n",
        "        video_name = frame_path.split('/')[-2]\n",
        "        video_meta = self._get_meta('video_name', video_name)\n",
        "        label = video_meta['label'].item()\n",
        "\n",
        "        frame = Image.open(frame_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            frame = self.transform(frame)\n",
        "        else:\n",
        "            frame = T.ToTensor()(frame)\n",
        "\n",
        "        return frame, label\n",
        "\n",
        "\n",
        "class FrameVideoDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir = '/work3/ppar/data/ucf101', split = 'train', transform = None, stack_frames = True):\n",
        "        self.video_paths = sorted(glob(f'{root_dir}/videos/{split}/*/*.avi'))\n",
        "        self.df = pd.read_csv(f'{root_dir}/metadata/{split}.csv')\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.stack_frames = stack_frames\n",
        "\n",
        "        self.video_names = self.df['video_name'].tolist()\n",
        "\n",
        "        self.n_sampled_frames = 10\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def _get_meta(self, attr, value):\n",
        "        return self.df.loc[self.df[attr] == value]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        video_name = video_path.split('/')[-1].split('.avi')[0]\n",
        "        video_meta = self._get_meta('video_name', video_name)\n",
        "        label = video_meta['label'].item()\n",
        "\n",
        "        video_frames_dir = self.video_paths[idx].split('.avi')[0].replace('videos', 'frames')\n",
        "        video_frames = self.load_frames(video_frames_dir)\n",
        "\n",
        "        if self.transform:\n",
        "            frames = [self.transform(frame) for frame in video_frames]\n",
        "        else:\n",
        "            frames = [T.ToTensor()(frame) for frame in video_frames]\n",
        "\n",
        "        if self.stack_frames:\n",
        "          frames = torch.stack(frames).permute(1, 0, 2, 3)\n",
        "          #frames 本身是之前frame的list. stack把这个list堆起来了\n",
        "          #permute的作用是重新排列维度的顺序。这里就是把T和C顺序互换了\n",
        "\n",
        "        return frames, label, video_name\n",
        "\n",
        "    def load_frames(self, frames_dir):\n",
        "        frames = []\n",
        "        for i in range(1, self.n_sampled_frames + 1):\n",
        "            frame_file = os.path.join(frames_dir, f\"frame_{i}.jpg\")\n",
        "            frame = Image.open(frame_file).convert(\"RGB\")\n",
        "            frames.append(frame)\n",
        "\n",
        "        return frames"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "    root_dir = root_dir\n",
        "\n",
        "    transform = T.Compose([T.Resize((64, 64)),T.ToTensor()])\n",
        "    frameimage_dataset = FrameImageDataset(root_dir=root_dir, split='val', transform=transform)\n",
        "    framevideostack_dataset = FrameVideoDataset(root_dir=root_dir, split='val', transform=transform, stack_frames = True)\n",
        "    framevideolist_dataset = FrameVideoDataset(root_dir=root_dir, split='val', transform=transform, stack_frames = False)\n",
        "\n",
        "\n",
        "    frameimage_loader = DataLoader(frameimage_dataset,  batch_size=8, shuffle=False)\n",
        "    framevideostack_loader = DataLoader(framevideostack_dataset,  batch_size=2, shuffle=False)\n",
        "    framevideolist_loader = DataLoader(framevideolist_dataset,  batch_size=2, shuffle=False)\n",
        "\n",
        "    print( \"Pure frame\"+ \"---\" * 45) # 无法补充时间维度信息了\n",
        "    # for frames, labels in frameimage_loader:\n",
        "    #     print(frames.shape, labels.shape) # [batch, channels, height, width]\n",
        "    frames, labels = next(iter(frameimage_loader))\n",
        "    print(frames.shape, labels.shape) # [batch, channels, height, width]\n",
        "\n",
        "\n",
        "    print( \"List frames in a video\" + \"---\" * 45)\n",
        "    # for video_frames, labels, video_names in framevideolist_loader:\n",
        "    #     for frame in video_frames: # loop through number of frames # 每个video_frames是按时间顺序的\n",
        "    #         print(frame.shape, labels.shape, video_names.shape)# [batch, channels, height, width]\n",
        "    video_frames, labels, video_names = next(iter(framevideolist_loader))\n",
        "\n",
        "    for frame in video_frames: # loop through number of frames # 每个video_frames是按时间顺序的\n",
        "        print(frame.shape, labels.shape, video_names)# [batch, channels, height, width]\n",
        "\n",
        "    print( \"Stack frames in a video\" + \"---\" * 45)\n",
        "    # for video_frames, labels in framevideostack_loader: # 每个video_frames是一个video按时间顺序的\n",
        "    #     print(video_frames.shape, labels.shape) # [batch, channels, number of frames, height, width]\n",
        "    video_frames, labels, video_names = next(iter(framevideostack_loader))\n",
        "    print(video_frames.shape, labels.shape, video_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qJuLJpA-SUe",
        "outputId": "d04c9c37-bc66-4939-f85c-1e24120ec49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pure frame---------------------------------------------------------------------------------------------------------------------------------------\n",
            "torch.Size([8, 3, 64, 64]) torch.Size([8])\n",
            "List frames in a video---------------------------------------------------------------------------------------------------------------------------------------\n",
            "torch.Size([2, 3, 64, 64]) torch.Size([2]) ('v_BodyWeightSquats_g03_c03', 'v_BodyWeightSquats_g05_c04')\n",
            "torch.Size([2, 3, 64, 64]) torch.Size([2]) ('v_BodyWeightSquats_g03_c03', 'v_BodyWeightSquats_g05_c04')\n",
            "torch.Size([2, 3, 64, 64]) torch.Size([2]) ('v_BodyWeightSquats_g03_c03', 'v_BodyWeightSquats_g05_c04')\n",
            "torch.Size([2, 3, 64, 64]) torch.Size([2]) ('v_BodyWeightSquats_g03_c03', 'v_BodyWeightSquats_g05_c04')\n",
            "torch.Size([2, 3, 64, 64]) torch.Size([2]) ('v_BodyWeightSquats_g03_c03', 'v_BodyWeightSquats_g05_c04')\n",
            "torch.Size([2, 3, 64, 64]) torch.Size([2]) ('v_BodyWeightSquats_g03_c03', 'v_BodyWeightSquats_g05_c04')\n",
            "torch.Size([2, 3, 64, 64]) torch.Size([2]) ('v_BodyWeightSquats_g03_c03', 'v_BodyWeightSquats_g05_c04')\n",
            "torch.Size([2, 3, 64, 64]) torch.Size([2]) ('v_BodyWeightSquats_g03_c03', 'v_BodyWeightSquats_g05_c04')\n",
            "torch.Size([2, 3, 64, 64]) torch.Size([2]) ('v_BodyWeightSquats_g03_c03', 'v_BodyWeightSquats_g05_c04')\n",
            "torch.Size([2, 3, 64, 64]) torch.Size([2]) ('v_BodyWeightSquats_g03_c03', 'v_BodyWeightSquats_g05_c04')\n",
            "Stack frames in a video---------------------------------------------------------------------------------------------------------------------------------------\n",
            "torch.Size([2, 3, 10, 64, 64]) torch.Size([2]) ('v_BodyWeightSquats_g03_c03', 'v_BodyWeightSquats_g05_c04')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Classification based on Per-frame models/late fusion/early fusion"
      ],
      "metadata": {
        "id": "2-25nilZ2o_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ldVtrXSA2ovv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"The code will run on GPU.\")\n",
        "else:\n",
        "    print(\"The code will run on CPU. Go to Edit->Notebook Settings and choose GPU as the hardware accelerator\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "ur-mwcsotdAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7088c5f9-bda9-427b-fe57-ef645c8b8836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The code will run on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FrameClassifier(nn.Module):\n",
        "  def __init__(self, num_classes=10, backbone_name=\"resnet18\"):\n",
        "    super().__init__()\n",
        "    self.backbone = getattr(models, backbone_name)(weights=None)\n",
        "    self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes) # 把resnet18 的fc层替换成一个空操作。\n",
        "\n",
        "  def forward(self, frames):\n",
        "    return self.backbone(frames)"
      ],
      "metadata": {
        "id": "m6yUtuIt64tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import log\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    total_samples = 0\n",
        "    for frames, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "        outputs = model(frames)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * frames.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_samples += frames.size(0)\n",
        "    return total_loss / total_samples, total_correct / total_samples\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device, desc=\"Evaluating\"):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for frames, labels, video_names in tqdm(dataloader, desc=desc, leave=False):\n",
        "            frames = frames.to(device)\n",
        "            labels = labels.to(device)\n",
        "            B, C, T, H, W = frames.shape\n",
        "            frames_reshaped = frames.permute(0, 2, 1, 3, 4).reshape(B*T, C, H, W) # batch*视频 维度合并为1维。从而并行求每帧的logiit\n",
        "\n",
        "            logits_reshaped = model(frames_reshaped)\n",
        "            logits = logits_reshaped.view(B, T, -1) # 恢复出视频维度。每行是一个视频\n",
        "            video_logits = logits.mean(dim=1) # 对每行（即一个视频）的每个class做求均值的操作。\n",
        "\n",
        "            preds = video_logits.argmax(dim=1) # 求出这行中得分最高的class\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "lTIX1vNDvgDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([\n",
        "    T.Resize((64, 64)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = FrameImageDataset(root_dir=root_dir, split='train', transform=transform)\n",
        "val_dataset = FrameVideoDataset(root_dir=root_dir, split='val', transform=transform, stack_frames=True)\n",
        "test_dataset = FrameVideoDataset(root_dir=root_dir, split='test', transform=transform, stack_frames=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "model = FrameClassifier(num_classes=10).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_acc = 0\n",
        "epoch_num = 30\n",
        "output_path = model_output + 'best_model.pth'\n",
        "for epoch in range(epoch_num):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "    val_acc = evaluate(model, val_loader, device, desc=\"Evaluating\")\n",
        "    print(f\"Epoch {epoch+1}/{epoch_num}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "       best_acc = val_acc\n",
        "       torch.save(model.state_dict(), output_path)\n",
        "\n",
        "best_model = FrameClassifier(num_classes=10)\n",
        "best_model.load_state_dict(torch.load(output_path))\n",
        "best_model.to(device)\n",
        "\n",
        "test_acc = evaluate(best_model, test_loader, device, desc=\"Testing\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "FpGn09wor4Jj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "89876018-11d8-4464-b10e-0404cbb1afe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30: Train Loss: 0.6846, Train Acc: 0.8042, Val Acc: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/30: Train Loss: 0.1052, Train Acc: 0.9743, Val Acc: 0.8667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/30: Train Loss: 0.0835, Train Acc: 0.9749, Val Acc: 0.8917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/30: Train Loss: 0.0588, Train Acc: 0.9858, Val Acc: 0.8500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/30: Train Loss: 0.0651, Train Acc: 0.9787, Val Acc: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/30: Train Loss: 0.0444, Train Acc: 0.9870, Val Acc: 0.9000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/30: Train Loss: 0.0408, Train Acc: 0.9888, Val Acc: 0.8917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/30: Train Loss: 0.0297, Train Acc: 0.9918, Val Acc: 0.8583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/30: Train Loss: 0.0453, Train Acc: 0.9870, Val Acc: 0.8667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/30: Train Loss: 0.0302, Train Acc: 0.9908, Val Acc: 0.8833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/30: Train Loss: 0.0370, Train Acc: 0.9892, Val Acc: 0.8833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/30: Train Loss: 0.0344, Train Acc: 0.9906, Val Acc: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/30: Train Loss: 0.0197, Train Acc: 0.9944, Val Acc: 0.8917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/30: Train Loss: 0.0046, Train Acc: 0.9996, Val Acc: 0.9083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/30: Train Loss: 0.0496, Train Acc: 0.9829, Val Acc: 0.8667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/30: Train Loss: 0.0233, Train Acc: 0.9938, Val Acc: 0.8917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/30: Train Loss: 0.0099, Train Acc: 0.9978, Val Acc: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/30: Train Loss: 0.0314, Train Acc: 0.9910, Val Acc: 0.9083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/30: Train Loss: 0.0104, Train Acc: 0.9978, Val Acc: 0.9000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/30: Train Loss: 0.0060, Train Acc: 0.9980, Val Acc: 0.8667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/30: Train Loss: 0.0187, Train Acc: 0.9946, Val Acc: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/30: Train Loss: 0.0282, Train Acc: 0.9906, Val Acc: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/30: Train Loss: 0.0064, Train Acc: 0.9980, Val Acc: 0.8833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/30: Train Loss: 0.0150, Train Acc: 0.9968, Val Acc: 0.9083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/30: Train Loss: 0.0079, Train Acc: 0.9978, Val Acc: 0.9167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/30: Train Loss: 0.0375, Train Acc: 0.9890, Val Acc: 0.9000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/30: Train Loss: 0.0083, Train Acc: 0.9976, Val Acc: 0.9000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/30: Train Loss: 0.0031, Train Acc: 0.9996, Val Acc: 0.9000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/30: Train Loss: 0.0008, Train Acc: 1.0000, Val Acc: 0.9083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/30: Train Loss: 0.0311, Train Acc: 0.9902, Val Acc: 0.8833\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/model_output/video_classification/best_model.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2148300233.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/model_output/video_classification/best_model.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Late & Early Fusion\n",
        "Late Fusion uses MLP"
      ],
      "metadata": {
        "id": "4_zEPMoo44Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoClassifier(nn.Module):\n",
        "  def __init__(self, num_classes=10, backbone_name=\"resnet18\", fusion='late'):\n",
        "    super().__init__()\n",
        "    assert fusion in ['late', 'early'], \"Fusion method should be 'late' or 'early'\"\n",
        "    self.fusion = fusion\n",
        "    backbone = getattr(models, backbone_name)(weights=None)\n",
        "    backbone.fc = nn.Identity()\n",
        "    self.backbone = backbone\n",
        "\n",
        "    if self.fusion == 'early':\n",
        "       self.classifier = nn.Linear(512, num_classes)\n",
        "    else:\n",
        "       self.temporal_mlp = nn.Sequential(\n",
        "           nn.Linear(512, 512),\n",
        "           nn.ReLU(inplace=True),\n",
        "           nn.Linear(512, 512),\n",
        "           nn.ReLU(inplace=True),\n",
        "       )\n",
        "       self.classifier = nn.Linear(512, num_classes)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "      if x.dim() != 5:\n",
        "         raise ValueError(f\"Expected input dim=5(B,C,T,H,W), got:{x.dim()}\")\n",
        "\n",
        "      B, C, T, H, W = x.shape\n",
        "      # if self.fusion == 'early':\n",
        "      #   x_early = x.view(B, C*T, H, W)\n",
        "      #   return logits\n",
        "      if self.fusion == 'late':\n",
        "         frames_all = x.permute(0,2,1,3,4).reshape(B*T, C, H, W)\n",
        "         feats_all = self.backbone(frames_all) # [B*T, 512]\n",
        "         feats = feats_all.view(B, T, -1) # [B, T, 512]\n",
        "\n",
        "         B_, T_, D = feats.shape\n",
        "         feats_flat = feats.view(B_*T_, D)\n",
        "         fused_flat = self.temporal_mlp(feats_flat)\n",
        "         fused = fused_flat.view(B_, T_, D)\n",
        "         video_feat = fused.mean(dim=1)\n",
        "\n",
        "         logits = self.classifier(video_feat)\n",
        "         return logits\n",
        "\n",
        "      else:\n",
        "        raise ValueError(f\"Invalid fusion method: {self.fusion}\")"
      ],
      "metadata": {
        "id": "YigfbVAn42--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss, total_correct, total_samples = 0,0,0\n",
        "    for frames, labels, *_ in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "        logits = model(frames)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch = frames.size(0)\n",
        "        total_loss += loss.item() * batch\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_samples += batch\n",
        "    return total_loss / total_samples, total_correct / total_samples\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, criterion, device, desc=\"Evaluating\"):\n",
        "    model.eval()\n",
        "    total_loss, total_correct, total_samples = 0,0,0\n",
        "    for frames, labels, *_ in tqdm(dataloader, desc=desc, leave=False):\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "\n",
        "        logits = model(frames)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        batch = frames.size(0)\n",
        "        total_loss += loss.item() * batch\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_samples += batch\n",
        "    return total_loss / total_samples, total_correct / total_samples"
      ],
      "metadata": {
        "id": "59XcxsTNCBAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([\n",
        "    T.Resize((64, 64)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = FrameVideoDataset(root_dir=root_dir, split='train', transform=transform, stack_frames=True)\n",
        "val_dataset = FrameVideoDataset(root_dir=root_dir, split='val', transform=transform, stack_frames=True)\n",
        "test_dataset = FrameVideoDataset(root_dir=root_dir, split='test', transform=transform, stack_frames=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "model = VideoClassifier(num_classes=10, fusion='late').to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_acc = 0\n",
        "epoch_num = 30\n",
        "output_path = model_output + 'best_late_fusion_model.pth'\n",
        "for epoch in range(epoch_num):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device, desc=\"Evaluating\")\n",
        "    print(f\"Epoch {epoch+1}/{epoch_num}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "       best_acc = val_acc\n",
        "       torch.save(model.state_dict(), output_path)\n",
        "\n",
        "best_model = VideoClassifier(num_classes=10, fusion='late')\n",
        "best_model.load_state_dict(torch.load(output_path))\n",
        "best_model.to(device)\n",
        "\n",
        "_, test_acc = evaluate(best_model, test_loader, criterion, device, desc=\"Testing\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edfzYwzk9vPT",
        "outputId": "40dc9dd9-2b5d-4c18-86ba-47d59b0f1a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  59%|█████▊    | 37/63 [23:43<16:22, 37.79s/it]"
          ]
        }
      ]
    }
  ]
}